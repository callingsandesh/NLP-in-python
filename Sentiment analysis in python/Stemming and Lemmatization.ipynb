{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9c3f96b3",
   "metadata": {},
   "source": [
    "## Stems and lemmas from GoT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3f59544",
   "metadata": {},
   "source": [
    "We have  a couple of sentences from George R.R. Martin's Game of Thrones.\n",
    "\n",
    "stems reduce a word to its root whereas lemmas produce an actual word. However, speed can differ significantly between the methods with stemming being much faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5ec153e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "GoT='Never forget what you are, for surely the world will not. Make it your strength. Then it can never be your weakness. Armour yourself in it, and it will never be used to hurt you.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d4975716",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the required packages from nltk\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk import word_tokenize\n",
    "\n",
    "porter = PorterStemmer()\n",
    "WNlemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Tokenize the GoT string\n",
    "tokens = word_tokenize(GoT) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2fc580f",
   "metadata": {},
   "source": [
    "## Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6352d3e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for stemming in seconds:  0.0009968280792236328\n",
      "Stemmed tokens:  ['never', 'forget', 'what', 'you', 'are', ',', 'for', 'sure', 'the', 'world', 'will', 'not', '.', 'make', 'it', 'your', 'strength', '.', 'then', 'it', 'can', 'never', 'be', 'your', 'weak', '.', 'armour', 'yourself', 'in', 'it', ',', 'and', 'it', 'will', 'never', 'be', 'use', 'to', 'hurt', 'you', '.']\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# Log the start time\n",
    "start_time = time.time()\n",
    "\n",
    "# Build a stemmed list\n",
    "stemmed_tokens = [porter.stem(token) for token in tokens]\n",
    "\n",
    "# Log the end time\n",
    "end_time = time.time()\n",
    "\n",
    "print('Time taken for stemming in seconds: ', end_time - start_time)\n",
    "print('Stemmed tokens: ', stemmed_tokens) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b95e40c",
   "metadata": {},
   "source": [
    "## Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a4a6ca15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for lemmatizing in seconds:  1.6366140842437744\n",
      "Lemmatized tokens:  ['Never', 'forget', 'what', 'you', 'are', ',', 'for', 'surely', 'the', 'world', 'will', 'not', '.', 'Make', 'it', 'your', 'strength', '.', 'Then', 'it', 'can', 'never', 'be', 'your', 'weakness', '.', 'Armour', 'yourself', 'in', 'it', ',', 'and', 'it', 'will', 'never', 'be', 'used', 'to', 'hurt', 'you', '.']\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# Log the start time\n",
    "start_time = time.time()\n",
    "\n",
    "# Build a lemmatized list\n",
    "lem_tokens = [WNlemmatizer.lemmatize(token) for token in tokens]\n",
    "\n",
    "# Log the end time\n",
    "end_time = time.time()\n",
    "\n",
    "print('Time taken for lemmatizing in seconds: ', end_time - start_time)\n",
    "print('Lemmatized tokens: ', lem_tokens) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeace06a",
   "metadata": {},
   "source": [
    "## Stemming Spanish reviews of amazon product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "705c7f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "reviews = pd.read_csv('amazon_reviews_sample.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "75ebd31e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langdetect import detect_langs\n",
    "languages = [] \n",
    "\n",
    "# Loop over the rows of the dataset and append  \n",
    "for row in range(len(reviews.review)):\n",
    "    languages.append(detect_langs(reviews.iloc[row, 2]))\n",
    "\n",
    "# Clean the list by splitting     \n",
    "languages = [str(lang).split(':')[0][1:] for lang in languages]\n",
    "# Assign the list to a new feature \n",
    "reviews['language'] = languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "033a9c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "non_english_reviews = reviews[reviews['language'] == 'es']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e309626d",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>score</th>\n",
       "      <th>review</th>\n",
       "      <th>language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1259</th>\n",
       "      <td>1259</td>\n",
       "      <td>1</td>\n",
       "      <td>La reencarnación vista por un científico: El ...</td>\n",
       "      <td>es</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1260</th>\n",
       "      <td>1260</td>\n",
       "      <td>1</td>\n",
       "      <td>Excelente Libro / Amazing book!!: Este libro ...</td>\n",
       "      <td>es</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1261</th>\n",
       "      <td>1261</td>\n",
       "      <td>1</td>\n",
       "      <td>Magnifico libro: Brian Weiss ha dejado una ma...</td>\n",
       "      <td>es</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1639</th>\n",
       "      <td>1639</td>\n",
       "      <td>1</td>\n",
       "      <td>El libro mas completo que existe para nosotra...</td>\n",
       "      <td>es</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1745</th>\n",
       "      <td>1745</td>\n",
       "      <td>1</td>\n",
       "      <td>Excelente!: Una excelente guía para todos aqu...</td>\n",
       "      <td>es</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2486</th>\n",
       "      <td>2486</td>\n",
       "      <td>1</td>\n",
       "      <td>Palabras de aliento para tu caminar con Dios:...</td>\n",
       "      <td>es</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2903</th>\n",
       "      <td>2903</td>\n",
       "      <td>1</td>\n",
       "      <td>fabuloso: mil gracias por el producto fabulos...</td>\n",
       "      <td>es</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3318</th>\n",
       "      <td>3318</td>\n",
       "      <td>1</td>\n",
       "      <td>Excelentes botas.. excelentes boots: Excelent...</td>\n",
       "      <td>es</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3694</th>\n",
       "      <td>3694</td>\n",
       "      <td>0</td>\n",
       "      <td>Why not Spanish ???: Alguien me puede decir p...</td>\n",
       "      <td>es</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4820</th>\n",
       "      <td>4820</td>\n",
       "      <td>1</td>\n",
       "      <td>La mejor película de Moore: A mi juicio, esta...</td>\n",
       "      <td>es</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5901</th>\n",
       "      <td>5901</td>\n",
       "      <td>1</td>\n",
       "      <td>Buen cargador: Product very good, I am of Ven...</td>\n",
       "      <td>es</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6234</th>\n",
       "      <td>6234</td>\n",
       "      <td>1</td>\n",
       "      <td>5+ stars. LO MEJOR DE LO QUE HE LEIDO EN MI V...</td>\n",
       "      <td>es</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7078</th>\n",
       "      <td>7078</td>\n",
       "      <td>1</td>\n",
       "      <td>Variedad: Bueno tener este album debido a su ...</td>\n",
       "      <td>es</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8018</th>\n",
       "      <td>8018</td>\n",
       "      <td>1</td>\n",
       "      <td>Exelente eleccion: Los mejores zapatos de fut...</td>\n",
       "      <td>es</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9265</th>\n",
       "      <td>9265</td>\n",
       "      <td>1</td>\n",
       "      <td>Excelente: Manu es una de los mejores cantant...</td>\n",
       "      <td>es</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9624</th>\n",
       "      <td>9624</td>\n",
       "      <td>0</td>\n",
       "      <td>baaaaaadddddddd bookkkkkkk: por favor no gast...</td>\n",
       "      <td>es</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0  score                                             review  \\\n",
       "1259        1259      1   La reencarnación vista por un científico: El ...   \n",
       "1260        1260      1   Excelente Libro / Amazing book!!: Este libro ...   \n",
       "1261        1261      1   Magnifico libro: Brian Weiss ha dejado una ma...   \n",
       "1639        1639      1   El libro mas completo que existe para nosotra...   \n",
       "1745        1745      1   Excelente!: Una excelente guía para todos aqu...   \n",
       "2486        2486      1   Palabras de aliento para tu caminar con Dios:...   \n",
       "2903        2903      1   fabuloso: mil gracias por el producto fabulos...   \n",
       "3318        3318      1   Excelentes botas.. excelentes boots: Excelent...   \n",
       "3694        3694      0   Why not Spanish ???: Alguien me puede decir p...   \n",
       "4820        4820      1   La mejor película de Moore: A mi juicio, esta...   \n",
       "5901        5901      1   Buen cargador: Product very good, I am of Ven...   \n",
       "6234        6234      1   5+ stars. LO MEJOR DE LO QUE HE LEIDO EN MI V...   \n",
       "7078        7078      1   Variedad: Bueno tener este album debido a su ...   \n",
       "8018        8018      1   Exelente eleccion: Los mejores zapatos de fut...   \n",
       "9265        9265      1   Excelente: Manu es una de los mejores cantant...   \n",
       "9624        9624      0   baaaaaadddddddd bookkkkkkk: por favor no gast...   \n",
       "\n",
       "     language  \n",
       "1259       es  \n",
       "1260       es  \n",
       "1261       es  \n",
       "1639       es  \n",
       "1745       es  \n",
       "2486       es  \n",
       "2903       es  \n",
       "3318       es  \n",
       "3694       es  \n",
       "4820       es  \n",
       "5901       es  \n",
       "6234       es  \n",
       "7078       es  \n",
       "8018       es  \n",
       "9265       es  \n",
       "9624       es  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "non_english_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8ded9213",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['la', 'reencarn', 'vist', 'por', 'un', 'cientif', ':', 'el', 'prim', 'libr', 'del', 'dr.', 'weiss', 'sig', 'siend', 'un', 'gran', 'libr', 'par', 'tod', 'aquell', 'a', 'quien', 'les', 'inquiet', 'el', 'tem', 'de', 'la', 'reencarn', ',', 'asi', 'no', 'cre', 'en', 'ella', '.']\n"
     ]
    }
   ],
   "source": [
    "# Import the required packages\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk import word_tokenize\n",
    "\n",
    "# Import the Spanish SnowballStemmer\n",
    "SpanishStemmer = SnowballStemmer(\"spanish\")\n",
    "\n",
    "# Create a list of tokens\n",
    "tokens = [word_tokenize(review) for review in non_english_reviews.review] \n",
    "# Stem the list of tokens\n",
    "stemmed_tokens = [[SpanishStemmer.stem(word) for word in token] for token in tokens]\n",
    "\n",
    "# Print the first item of the stemmed tokenss\n",
    "print(stemmed_tokens[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3d9db1e",
   "metadata": {},
   "source": [
    "## STEM FROM TWEETS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1d3bfb43",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_raw = pd.read_csv('tweets.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f624ad16",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = tweets_raw.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8ca9e2f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['@', 'virginamerica', 'what', '@', 'dhepburn', 'said', '.']\n"
     ]
    }
   ],
   "source": [
    "# Import the function to perform stemming\n",
    "from nltk.stem import PorterStemmer\n",
    "#from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk import word_tokenize\n",
    "\n",
    "# Call the stemmer\n",
    "porter = PorterStemmer()\n",
    "\n",
    "# Transform the array of tweets to tokens\n",
    "tokens = [word_tokenize(tweet) for tweet in tweets]\n",
    "# Stem the list of tokens\n",
    "stemmed_tokens = [[porter.stem(word) for word in tweet] for tweet in tokens] \n",
    "# Print the first element of the list\n",
    "print(stemmed_tokens[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "951c4b6b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "065cae8b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
